# Lab 04 ‚Äî –î–µ—Ä–µ–≤—å—è —Ä–µ—à–µ–Ω–∏–π –∏ –∞–Ω—Å–∞–º–±–ª–∏
# Lab 04 ‚Äî Decision Trees and Ensembles

---

## üá∑üá∫ –û–ø–∏—Å–∞–Ω–∏–µ –∑–∞–¥–∞–Ω–∏—è

### –¶–µ–ª—å —Ä–∞–±–æ—Ç—ã
–ò–∑—É—á–∏—Ç—å –º–µ—Ç–æ–¥—ã –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–µ—Ä–µ–≤—å–µ–≤ —Ä–µ—à–µ–Ω–∏–π –∏ –∞–Ω—Å–∞–º–±–ª–µ–π, –≤–∫–ª—é—á–∞—è –±—ç–≥–≥–∏–Ω–≥, —Å—Ç–µ–∫–∏–Ω–≥, —Å–ª—É—á–∞–π–Ω—ã–π –ª–µ—Å –∏ XGBoost, –∞ —Ç–∞–∫–∂–µ –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –≤–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤.

### –ó–∞–¥–∞—á–∏
1. –ü—Ä–æ–≤–µ—Å—Ç–∏ –∞–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö (EDA).
2. –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∞—Ç—å –¥–∞–Ω–Ω—ã–µ (–º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ, –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è, –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤).
3. –ü–æ—Å—Ç—Ä–æ–∏—Ç—å –¥–µ—Ä–µ–≤–æ —Ä–µ—à–µ–Ω–∏–π:
   - –≤–∏–∑—É–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –¥–µ—Ä–µ–≤–æ,
   - –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç—å –≤–ª–∏—è–Ω–∏–µ –≥–ª—É–±–∏–Ω—ã –¥–µ—Ä–µ–≤–∞ –Ω–∞ –∫–∞—á–µ—Å—Ç–≤–æ –º–æ–¥–µ–ª–∏,
   - –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –æ–ø—Ç–∏–º–∞–ª—å–Ω—É—é –≥–ª—É–±–∏–Ω—É.
4. –ù–∞—Å—Ç—Ä–æ–∏—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–µ—Ä–µ–≤–∞ —Å –ø–æ–º–æ—â—å—é GridSearchCV.
5. –û–±—É—á–∏—Ç—å –∞–Ω—Å–∞–º–±–ª–∏:
   - BaggingClassifier —Å –≤—ã–±—Ä–∞–Ω–Ω–æ–π –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª—å—é,
   - StackingClassifier —Å –≤—ã–±—Ä–∞–Ω–Ω—ã–º–∏ –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–∏–º–∏ –º–æ–¥–µ–ª—è–º–∏.
6. –û–±—É—á–∏—Ç—å RandomForestClassifier –∏ –Ω–∞—Å—Ç—Ä–æ–∏—Ç—å –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã —Å –ø–æ–º–æ—â—å—é GridSearchCV.
7. –û–±—É—á–∏—Ç—å XGBoost (—Ç–æ–ª—å–∫–æ —Å —á–∏—Å–ª–æ–≤—ã–º–∏ –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏) –∏ –≤—ã—á–∏—Å–ª–∏—Ç—å F-score –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤.
8. –°–¥–µ–ª–∞—Ç—å –≤—ã–≤–æ–¥—ã –ø–æ –ø—Ä–æ–¥–µ–ª–∞–Ω–Ω–æ–π —Ä–∞–±–æ—Ç–µ.

### –¢—Ä–µ–±–æ–≤–∞–Ω–∏—è –∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—É
- EDA —Å –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è–º–∏.
- –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –¥–µ—Ä–µ–≤–∞ —Ä–µ—à–µ–Ω–∏–π –∏ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤.
- –¢–∞–±–ª–∏—Ü—ã –∏ –≥—Ä–∞—Ñ–∏–∫–∏ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤.
- –í—ã–≤–æ–¥—ã –ø–æ –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–º –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º –∏ —Ä–∞–±–æ—Ç–µ –º–æ–¥–µ–ª–µ–π.

---

## üá¨üáß Task description

### Goal
Study classification methods based on decision trees and ensembles, including bagging, stacking, Random Forest, and XGBoost, and analyze feature importance.

### Tasks
1. Perform exploratory data analysis (EDA).
2. Preprocess data (scaling, normalization, encode categorical features).
3. Build a decision tree:
   - visualize the tree,
   - investigate the effect of tree depth on model quality,
   - determine the optimal depth.
4. Tune tree parameters with GridSearchCV.
5. Train ensembles:
   - BaggingClassifier with chosen base model,
   - StackingClassifier with selected classical models.
6. Train RandomForestClassifier and tune hyperparameters with GridSearchCV.
7. Train XGBoost (numerical features only) and compute F-score for feature importance.
8. Draw conclusions based on the results.

### Expected results
- EDA with explanations.
- Decision tree visualization.
- Tables and plots of feature importance.
- Conclusions on optimal parameters and model performance.
