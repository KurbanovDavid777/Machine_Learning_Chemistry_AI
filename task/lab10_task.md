\# Lab 10 ‚Äî Transformer –¥–ª—è —Ç–µ–∫—Å—Ç–æ–≤–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏

\# Lab 10 ‚Äî Transformer for Text Classification



---



\## üá∑üá∫ –û–ø–∏—Å–∞–Ω–∏–µ –∑–∞–¥–∞–Ω–∏—è



\### –¶–µ–ª—å —Ä–∞–±–æ—Ç—ã

–†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã Transformer —Å –Ω—É–ª—è –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞ –∏ –ø–æ—Å—Ç—Ä–æ–∏—Ç—å –ø–∞–π–ø–ª–∞–π–Ω –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏.



\### –ó–∞–¥–∞—á–∏

1\. –í—ã–±—Ä–∞—Ç—å –ª—é–±–æ–π –¥–∞—Ç–∞—Å–µ—Ç –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞ –∏ —Å–¥–µ–ª–∞—Ç—å –µ–≥–æ –±–∞–∑–æ–≤–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ.

2\. –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å —Å–ª–µ–¥—É—é—â–∏–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã Transformer:

&nbsp;  - MultiHeadAttention,

&nbsp;  - PositionalEncoding (cosine),

&nbsp;  - TransformerEncoderLayer,

&nbsp;  - TransformerEncoder,

&nbsp;  - TransformerClassifier.

3\. –î–ª—è –Ω–µ–∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–æ–Ω–Ω—ã—Ö –∑–∞–¥–∞—á —Ä–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å –∞–Ω–∞–ª–æ–≥–∏—á–Ω—ã–π Transformer<Task> –º–æ–¥—É–ª—å.

4\. –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å –ø–∞–π–ø–ª–∞–π–Ω –æ–±—É—á–µ–Ω–∏—è –∏ –æ–±—É—á–∏—Ç—å –º–æ–¥–µ–ª—å –¥–ª—è —Ç–µ–∫—Å—Ç–æ–≤–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏.

5\. –ü—Ä–æ–¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä–æ–≤–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –º–æ–¥–µ–ª–∏.

6\. –†–∞–∑—Ä–µ—à–µ–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å:

&nbsp;  - –≥–æ—Ç–æ–≤—ã–π —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä,

&nbsp;  - –±–∞–∑–æ–≤—ã–µ —Å–ª–æ–∏ PyTorch,

&nbsp;  - optimizer –∏ —Ñ—É–Ω–∫—Ü–∏–∏ –º–µ—Ç—Ä–∏–∫.

7\. –ù–µ–ª—å–∑—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å:

&nbsp;  - –≥–æ—Ç–æ–≤—ã–π —Ç–µ–∫—Å—Ç–æ–≤—ã–π —ç–º–±–µ–¥–¥–µ—Ä,

&nbsp;  - –≥–æ—Ç–æ–≤—ã–µ SDPA –∏–ª–∏ –±–ª–æ–∫–∏ Transformer.

8\. –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:

&nbsp;  - –≤—ã–±—Ä–∞—Ç—å –¥–∞—Ç–∞—Å–µ—Ç, –∫–æ—Ç–æ—Ä—ã–π –º–æ–∂–Ω–æ –±—É–¥–µ—Ç —Ä–∞—Å—à–∏—Ä–∏—Ç—å –≤ —Å–ª–µ–¥—É—é—â–µ–π –ª–∞–±–æ—Ä–∞—Ç–æ—Ä–Ω–æ–π –¥–ª—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–π –∑–∞–¥–∞—á–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏—è –ø–æ —Ñ–æ—Ç–æ –ª–∏—Ü–∞).



\### –¢—Ä–µ–±–æ–≤–∞–Ω–∏—è –∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—É

\- –†–µ–∞–ª–∏–∑–∞—Ü–∏—è –≤—Å–µ—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ Transformer.

\- –û–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ –≤—ã–±–æ—Ä–∞ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã.

\- –ú–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –º–æ–¥–µ–ª–∏.

\- –í—ã–≤–æ–¥—ã –ø–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º —Ä–∞–±–æ—Ç—ã.



---



\## üá¨üáß Task description



\### Goal

Implement Transformer components from scratch for text classification and build a training pipeline.



\### Tasks

1\. Select any text classification dataset and provide a basic description.

2\. Implement the following Transformer components:

&nbsp;  - MultiHeadAttention,

&nbsp;  - PositionalEncoding (cosine),

&nbsp;  - TransformerEncoderLayer,

&nbsp;  - TransformerEncoder,

&nbsp;  - TransformerClassifier.

3\. For non-classification tasks, implement a corresponding Transformer<Task> module.

4\. Implement a training pipeline and train the text classification model.

5\. Demonstrate the model results.

6\. Allowed:

&nbsp;  - pre-built tokenizer,

&nbsp;  - basic PyTorch layers,

&nbsp;  - optimizer and metric functions.

7\. Not allowed:

&nbsp;  - pre-built text embedder,

&nbsp;  - ready SDPA or Transformer blocks.

8\. Recommendation:

&nbsp;  - choose a dataset that can be extended in the next lab for a multimodal task (e.g., facial expression classification).



\### Expected results

\- Full implementation of Transformer components.

\- Justification of architecture choices.

\- Model quality metrics.

\- Clear conclusions based on the results.



